<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Guillem Brasó</title>
  
  <meta name="author" content="Guillem Brasó">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/guillembraso.png" src="images/guillembraso.png">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Guillem Brasó</name>
              </p>
              <p>I am a second year PhD candidate working with <a href="https://dvl.in.tum.de/team/lealtaixe/">Laura Leal-Taixé</a> at the <a href="https://dvl.in.tum.de/">Dynamic Vision and Learning </a> group,  <a href="https://www.tum.de/">TU Munich</a>.
              </p>
              <p>
                Before starting my PhD, I obtained a Master's in Mathematics from <a href="https://www.tum.de/">TU Munich</a>, while working on Multi-Object Tracking at my current lab. Before that, I got a Bachelor in Mathematics from the <a href="https://www.ub.edu/web/portal/en/">University of Barcelona</a>. During my Bsc, I worked with <a href="https://algorismes.github.io/">Jordi Vitrià</a> on explainable machine learning.
              </p>

              <p>
                In my free time, I like to climb, play the piano, and cook.
              </p>
              <p style="text-align:center">
                <a href="mailto:gbrasoa@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/GuillemBrasoCVOct2022.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.de/citations?user=0cXSWzcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/guillembraso">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/guillembraso/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/guillembraso.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/guillembraso.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                      <table style="width:100%; text-align: left;">
                        <tr>
                          <td> <strong>[September 2022]</strong> </td>
                          <td>  <a href="https://link.springer.com/article/10.1007/s11263-022-01678-6"> Our journal extension </a> of <a href="https://arxiv.org/abs/1912.07515"> MPNTrack </a> is accepted to <a href="https://nips.cc/"> IJCV!</a> </td>
                        </tr>

                        <tr>
                          <td> <strong>[September 2022]</strong> </td>
                          <td>  <a href="https://arxiv.org/abs/2210.05657"> Our paper </a> on fully connected layers for low-data regimes is accepted to <a href="https://nips.cc/"> NeurIPS 2022!</a> </td>
                        </tr>
                        <tr>
                          <td> <strong>[July 2022]</strong> </td>
                          <td> <a href="https://polarmot.github.io/"> PolarMOT</a>, our new geometry-based 3D tracker, is accepted to <a href="https://eccv2022.ecva.net/"> ECCV 2022!</a> </td>
                        </tr>

                        <tr>
                          <td> <strong>[May 2022]</strong> </td>
                          <td> I received a <a href="https://cvpr2022.thecvf.com/outstanding-reviewers"> CVPR 2022 outstanding reviewer</a> award!</td>
                        </tr>


                          <tr>
                              <td> <strong>[March 2022]</strong> </td>
                              <td> We are organizing the <a href="https://motchallenge.net/workshops/bmtt2022">BMTT workshop</a> at <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>. Check out our synth2real chalenges for tracking!</td>
                          </tr>
                          <tr>
                            <td> <strong>[March 2022]</strong> </td>
                            <td> Just created this website!</td>
                        </tr>
                      </table>
                  </p>
              </td>
          </tr>
      </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
              I have a general interest in Machine Learning and Computer Vision. Some of the tasks I focus on are  detection, segmentation, tracking and human pose estimation. I am also broadly interested in leveraging ideas from classical graph-based approaches and optimization in combination with deep learning to solve vision problems.
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mots_nmp.png" alt="blind-date" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/article/10.1007/s11263-022-01678-6">
                <papertitle>Multi-Object Tracking and Segmentation via Neural Message Passing </papertitle>
              </a>
              <br>
              <strong>Guillem Brasó*</strong>, Orçun Çetintas*, Laura Leal-Taixé 
              <br>
              <strong>IJCV 2022 </strong>
              <p> We build upon our neural solver for multi-object tracking to develop a joint model for tracking and segmentation with neural message passing.
              Our main contribution is an attention-based message passing framework that operates over segmentation masks and allows us to exploit synergies between data association and segmentation and SOTA results across
              multiple datasets.  
              </p>

                                                  
              <a href="https://arxiv.org/abs/2207.07454?context=cs">paper </a> |                                     
              <a href="https://github.com/ocetintas/MPNTrackSeg" type="text/html">code</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fc_layers.png" alt="blind-date" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.05657">
                <papertitle>The Unreasonable Effectiveness of Fully-Connected Layers for Low-Data Regimes</papertitle>
              </a>
              <br>
              Peter Kocsis, Peter Súkeník, <strong>Guillem Brasó</strong>, Matthias Niessner, Laura Leal-Taixé, Ismail Elezi 
              <br>
              <strong>NeurIPS 2022 </strong>
              <p> We show that augmenting modern classification architectures with a few fully connected layers during training significantly improves their 
                generalization performance on low-data regimes. We propose a self-distillation mechanism that allows us to keep the number of parameter constant at test-time, 
                while yielding universal improvements across backbones in datasets. </p>

                                                  
              <a href="https://arxiv.org/abs/2210.05657">paper</a> |                                     
              <a href="https://peter-kocsis.github.io/LowDataGeneralization/">website</a> 
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/polarmot.png" alt="blind-date" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2208.01957">
                <papertitle>PolarMot: How Far Can Geometric Relations Take Us in 3D Multi-Object Tracking?</papertitle>
              </a>
              <br>
             Alexandr Kim, <strong>Guillem Brasó</strong>, Aljosa Osep, Laura Leal-Taixé 
              <br>
              <strong>ECCV 2022 </strong>
              <p> We propose an appearance-free 3D multi-object tracker that learns to associate 3D bounding boxes over time based on their relative geometric features and spatio-temporal relationships. Our model is light-weight and generalizes remarkably well among different locations and dataset.</p>

                                                  
              <a href="https://arxiv.org/abs/2208.01957">paper</a> |                                     
              <a href="https://polarmot.github.io/">website</a> |
              <a href="https://github.com/aleksandrkim61/PolarMOT" type="text/html">code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/centergroup.png" alt="blind-date" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2110.05132">
                <papertitle>The Center of Attention: Center-Keypoint Attention for Multi-Person Pose Estimation</papertitle>
              </a>
              <br>
             <strong>Guillem Brasó</strong>, Nikita Kister, Laura Leal-Taixé 
              <br>
              <strong>ICCV 2021 </strong>
              <p> We propose an multi-head attention-based framework for end-to-end bottom-up multi-person pose estimation. Our main idea is to detect both keypoints and object centers, and use cross-attention among them to group keypoints into human poses.</p>

                                                  
              <a href="https://arxiv.org/abs/2110.05132">paper</a> |                                     
              <a href="https://www.youtube.com/watch?v=i4zFdz-ZzjM">video</a> |
              <a href="https://github.com/dvl-tum/center-group" type="text/html">code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/motsynth_teaser2.png" alt="blind-date" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2108.09518"">
                <papertitle>MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?</papertitle>
              </a>
              <br>
              Matteo Fabbri, <strong>Guillem Brasó</strong>, Gianluca Maugeri, Orcun Cetintas, Riccardo Gasparini, Aljosa Osep, Simone Calderara, Laura Leal-Taixe, Rita Cucchiara
              <br>
              <strong>ICCV 2021 </strong>
              <p> We introduce <em>MOTSynth</em>, a large-scale synthetic dataset for pedestrian tracking, detection, segmentation and 2D/3D keypoint estimation. We show that trackers and detectors trained on our dataset achieve impressive generalization results when tested on real data.</p>
          
              <a href="https://arxiv.org/abs/2108.09518">paper</a> |                                     
              <a href="https://motchallenge.net/data/MOTSynth-MOT-CVPR22/">dataset</a> |   
              <a href="https://www.youtube.com/watch?v=dc_Z1iCceL4">video</a> |
              <a href="https://github.com/dvl-tum/motsynth-baselines" type="text/html">code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/neuralsolver.png" alt="blind-date" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1912.07515">
                <papertitle> Learning a Neural Solver for Multiple Object Tracking</papertitle>
              </a>
              <br>
             <strong>Guillem Brasó</strong>, Laura Leal-Taixé 
              <br>
              <strong>CVPR 2020 </strong> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <p> Inspired by classical graph-based multi-object tracking methods, we propose a neural message passing framework for data association in multi-object tracking. We present the first graph-structured deep learning module for tracking, and achieve SOTA results on the MOTChallenge benchmarks.</p>

                                                  
              <a href="https://arxiv.org/abs/1912.07515">paper</a> |                                     
              <a href="https://www.youtube.com/watch?v=YWEirYMaLWc">video</a> |
              <a href="https://github.com/dvl-tum/mot_neural_solver" type="text/html">code</a>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                      This website's source code is from <a href="https://jonbarron.info/">Jon Barron</a>.
                  </p>
              </td>
          </tr>
      </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
